{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "from data_formatters.dubosson2018 import *\n",
    "from dataset import TSDataset\n",
    "from conf import Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code walk-through\n",
    "\n",
    "The major parts of the code that need to be defined for each data set are:\n",
    "1. config file in `.yaml` format,\n",
    "2. data formatter script.\n",
    "\n",
    "For now, you can study the `electricity.yaml` example for a look of what a config file should feel like. You can skip the hyperparam defintions and the model parameters. The main focus would be on defining the dataset parameters. \n",
    "\n",
    "We do not intereact with `.yaml` in a direct way but instead though `Conf` class, which handles the following:\n",
    "1. defines some defaults if not specified in `.yaml`,\n",
    "2. sets save paths,\n",
    "3. allows for nice colored printing.\n",
    "\n",
    "Technically, we could doo all of this in the `.yaml` file directly. However, then every time we re-run the experiment, we would have to manually modify the `.yaml` file to reset save paths and redefine some variables, which would be inconvenient.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the config file, setting the experiment name, and the seed for random pre-processing parts (like splitting)\n",
    "cnf = Conf(conf_file_path='./conf/dubosson.yaml', seed=15, exp_name=\"Dubosson\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default configuration parameters: \n",
      "\u001b[34mLR\u001b[0m\u001b[31m: \u001b[0m\u001b[35m0.001\u001b[0m\n",
      "\u001b[34mEPOCHS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m20\u001b[0m\n",
      "\u001b[34mN_WORKERS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m0\u001b[0m\n",
      "\u001b[34mBATCH_SIZE\u001b[0m\u001b[31m: \u001b[0m\u001b[35m64\u001b[0m\n",
      "\u001b[34mQUANTILES\u001b[0m\u001b[31m: \u001b[0m\u001b[35m[0.1, 0.5, 0.9]\u001b[0m\n",
      "\u001b[34mDS_NAME\u001b[0m\u001b[31m: \u001b[0m\u001b[33mdubosson2018\u001b[0m\n",
      "\u001b[34mALL_PARAMS\u001b[0m\u001b[31m: \u001b[0m\u001b[35m{'ds_name': 'dubosson2018', 'data_csv_path': './raw_data/Dubosson2018_processed.csv', 'index_col': -1, 'total_time_steps': 192, 'num_encoder_steps': 168, 'max_samples': 5000, 'batch_size': 64, 'device': 'cuda', 'lr': 0.001, 'num_epochs': 20, 'n_workers': 0, 'model': 'transformer', 'loader': 'base', 'quantiles': [0.1, 0.5, 0.9], 'batch_first': True, 'early_stopping_patience': 5, 'hidden_layer_size': 160, 'stack_size': 1, 'dropout_rate': 0.1, 'max_gradient_norm': 0.01, 'num_heads': 4, 'd_model': 64, 'q': 16, 'v': 16, 'h': 4, 'N': 2, 'attention_size': 0, 'dropout': 0.1, 'pe': 'original', 'chunk_mode': 'None', 'd_input': 5, 'd_output': 3}\u001b[0m\n",
      "\u001b[34mEXP_LOG_PATH\u001b[0m\u001b[31m: \u001b[0m\u001b[33m./log/transformer/Dubosson/10/07/2022.10:34:57\u001b[0m\n",
      "\u001b[34mDEVICE\u001b[0m\u001b[31m: \u001b[0m\u001b[33mcpu\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# lets print out the config file\n",
    "print(f'\\nDefault configuration parameters: \\n{cnf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the data formatter. This is the part that should handle:\n",
    "1. loading the data and setting types,\n",
    "2. splitting the data into train / val / test sets,\n",
    "3. setting scalers and encoders for numerical / categorical variables resp.\n",
    "\n",
    "We are going to leave parts 2-3 for the future exploration. Now, let's focus on loading and settting the types for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the data fromatter directly\n",
    "data_formatter = DubossonFormatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's work with the `TSDataset` class. This is the main part of the code as it aligns all of our previous steps. In the end, it is the `TSDataset` that is going to call the splitters, scalers, and encoders. **Importatnly** the model is only going to interact with the data through this class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "# available segments=6228\n",
      "Extracting 5000 samples out of 6228\n",
      "1000 of 5000 samples done...\n",
      "2000 of 5000 samples done...\n",
      "3000 of 5000 samples done...\n",
      "4000 of 5000 samples done...\n",
      "5000 of 5000 samples done...\n"
     ]
    }
   ],
   "source": [
    "# we are going to pass our data formatter and the config file to the TSDataset class\n",
    "dataset = TSDataset(cnf, data_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #0: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #1: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #2: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #3: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #4: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #5: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #6: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #7: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #8: x.shape=(192, 2), y.shape=(192, 1)\n",
      "Example #9: x.shape=(192, 2), y.shape=(192, 1)\n"
     ]
    }
   ],
   "source": [
    "# now let's see how we can sample minibatches from our dataset that we can then pass to the model to train on\n",
    "for i in range(10):\n",
    "    # 192 x ['power_usage', 'hour', 'day_of_week', 'hours_from_start', 'categorical_id']\n",
    "    x = dataset[i]['inputs']\n",
    "    # 24 x ['power_usage']\n",
    "    y = dataset[i]['outputs']\n",
    "    print(f'Example #{i}: x.shape={x.shape}, y.shape={y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>segment</th>\n",
       "      <th>time</th>\n",
       "      <th>gl</th>\n",
       "      <th>segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5810</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-01 11:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-01 11:50:00</td>\n",
       "      <td>69.12</td>\n",
       "      <td>7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-01 11:55:00</td>\n",
       "      <td>72.72</td>\n",
       "      <td>7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5813</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-01 12:00:00</td>\n",
       "      <td>76.68</td>\n",
       "      <td>7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5814</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-01 12:05:00</td>\n",
       "      <td>82.44</td>\n",
       "      <td>7_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6794</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-10-05 03:20:00</td>\n",
       "      <td>199.80</td>\n",
       "      <td>7_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6795</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-10-05 03:25:00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>7_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6796</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-10-05 03:30:00</td>\n",
       "      <td>196.20</td>\n",
       "      <td>7_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6797</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-10-05 03:35:00</td>\n",
       "      <td>194.40</td>\n",
       "      <td>7_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6798</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2014-10-05 03:40:00</td>\n",
       "      <td>194.40</td>\n",
       "      <td>7_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  segment                time      gl segment_id\n",
       "5810   7        1 2014-10-01 11:45:00     NaN        7_1\n",
       "5811   7        1 2014-10-01 11:50:00   69.12        7_1\n",
       "5812   7        1 2014-10-01 11:55:00   72.72        7_1\n",
       "5813   7        1 2014-10-01 12:00:00   76.68        7_1\n",
       "5814   7        1 2014-10-01 12:05:00   82.44        7_1\n",
       "...   ..      ...                 ...     ...        ...\n",
       "6794   7        2 2014-10-05 03:20:00  199.80        7_2\n",
       "6795   7        2 2014-10-05 03:25:00  198.00        7_2\n",
       "6796   7        2 2014-10-05 03:30:00  196.20        7_2\n",
       "6797   7        2 2014-10-05 03:35:00  194.40        7_2\n",
       "6798   7        2 2014-10-05 03:40:00  194.40        7_2\n",
       "\n",
       "[989 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.data\n",
    "df[df.id == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct list of segment indices by subject ###\n",
    "\n",
    "L = dict()\n",
    "segment = []\n",
    "for i in range(len(df)):\n",
    "    # get row information\n",
    "    row = df.iloc[i]\n",
    "    prev_row = df.iloc[0] if i == 0 else df.iloc[i-1]\n",
    "    \n",
    "    # check for change in subject or segment\n",
    "    if row.id != prev_row.id or row.segment != prev_row.segment:\n",
    "        if prev_row.id not in L:\n",
    "            L[prev_row.id] = [segment]\n",
    "        else:\n",
    "            L[prev_row.id].append(segment)\n",
    "        segment = []\n",
    "    \n",
    "    segment.append(i)\n",
    "    \n",
    "    # edge case: once at end of data, need to append final segment\n",
    "    if i == len(df)-1:\n",
    "        if prev_row.id not in L:\n",
    "            L[prev_row.id] = [segment]\n",
    "        else:\n",
    "            L[prev_row.id].append(segment)\n",
    "\n",
    "# Access subject segment indices using L[1][0] for subject 1 segment 1..., L[7][1] for subject 7 segment 2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments per Subject\n",
      "Subject 1: 1\n",
      "Subject 2: 1\n",
      "Subject 3: 1\n",
      "Subject 4: 1\n",
      "Subject 5: 1\n",
      "Subject 6: 1\n",
      "Subject 7: 2\n",
      "Subject 8: 1\n",
      "\n",
      "Points per Segment\n",
      "Subject 1 (Segment 1): 1413\n",
      "Subject 2 (Segment 1): 1056\n",
      "Subject 3 (Segment 1): 183\n",
      "Subject 4 (Segment 1): 969\n",
      "Subject 5 (Segment 1): 909\n",
      "Subject 6 (Segment 1): 1280\n",
      "Subject 7 (Segment 1): 612\n",
      "Subject 7 (Segment 2): 377\n",
      "Subject 8 (Segment 1): 1140\n"
     ]
    }
   ],
   "source": [
    "### Count number of segments ###\n",
    "print(\"Segments per Subject\")\n",
    "for subject in L:\n",
    "    num_segments = len(L[subject])\n",
    "    print(f\"Subject {subject}: {num_segments}\")\n",
    "    \n",
    "print()\n",
    "    \n",
    "print(\"Points per Segment\")\n",
    "for subject in L:\n",
    "    for i in range(len(L[subject])):\n",
    "        total_pts = len(L[subject][i])\n",
    "        print(f\"Subject {subject} (Segment {i+1}): {total_pts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "### Split Function ###\n",
    "# length segments: 12 hours of observation at 5 minutes frequency\n",
    "# 12 * 60 = 720 minutes\n",
    "# 720 / 5 = 144 5 minute intervals = 144 points\n",
    "_split_params = {\n",
    "  'test_percent_subjects': 0.1,\n",
    "  'test_length_segment': 144,\n",
    "  'val_length_segment': 144,\n",
    "  'min_drop_length': 144\n",
    "}\n",
    "\n",
    "# Get size of test set\n",
    "test_set = []\n",
    "number_of_subject = len(L)\n",
    "test_count = math.ceil(number_of_subject * _split_params['test_percent_subjects'])\n",
    "\n",
    "# add test set data\n",
    "for i in range(test_count):\n",
    "    subjects = list(L.keys())\n",
    "    curr_subject = L[subjects[i]]\n",
    "    for segment in curr_subject:\n",
    "        for idx in segment:\n",
    "            test_set.append(idx)\n",
    "\n",
    "# remove test set from L\n",
    "subjects = list(L.keys())\n",
    "for i in range(test_count):\n",
    "    del L[subjects[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "validation = []\n",
    "\n",
    "# iterate through L\n",
    "# check that segment has length >= min_drop_length + val_length_segment + test_length_segment\n",
    "for subject in L:\n",
    "    for segment in L[subject]:\n",
    "        if len(segment) >= _split_params['min_drop_length'] + _split_params['val_length_segment'] + _split_params['test_length_segment']:\n",
    "            train_set.append(segment[:(-(_split_params['val_length_segment'] + _split_params['test_length_segment']))])\n",
    "            validation.append(segment[-(_split_params['val_length_segment'] + _split_params['test_length_segment']):-_split_params['test_length_segment']])\n",
    "            test_set.append(segment[-_split_params['test_length_segment']:])\n",
    "        else:\n",
    "            train_set.append(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten sets\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "train_set = flatten(train_set)\n",
    "validation = flatten(validation)\n",
    "\n",
    "# Add to dictionary for access\n",
    "datasets = {'test': test_set, 'train': train_set, 'validation': validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419\n",
      "4798\n",
      "864\n"
     ]
    }
   ],
   "source": [
    "print(len(datasets['test']))\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
